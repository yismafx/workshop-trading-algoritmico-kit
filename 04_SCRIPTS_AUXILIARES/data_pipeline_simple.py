"""
ğŸ“Š DATA PIPELINE SIMPLE - Workshop Trading AlgorÃ­tmico

Script bÃ¡sico pero profesional para descargar, limpiar y validar datos histÃ³ricos
de mercado usando yfinance (gratis y sin API key).

CaracterÃ­sticas:
- âœ… Descarga datos de acciones/ETFs/criptos
- âœ… Limpia datos (elimina NaN, duplicados, gaps)
- âœ… Valida calidad de datos
- âœ… Guarda en CSV para reutilizaciÃ³n
- âœ… Maneja errores comunes

Autor: Workshop Trading AlgorÃ­tmico Aumentado con IA Generativa
VersiÃ³n: 1.0 (PÃºblico)
Fecha: 20 de noviembre de 2025
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')  # Silenciar warnings de yfinance

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DataConfig:
    """ConfiguraciÃ³n centralizada del pipeline de datos"""
    
    # Tickers a descargar (ejemplos)
    TICKERS = ['SPY', 'QQQ', 'TLT']  # ETFs populares
    
    # Rango de fechas
    START_DATE = '2020-01-01'
    END_DATE = datetime.now().strftime('%Y-%m-%d')  # Hasta hoy
    
    # Timeframe
    INTERVAL = '1d'  # Opciones: 1m, 5m, 15m, 1h, 1d, 1wk, 1mo
    
    # Output
    OUTPUT_DIR = './data/'  # Carpeta donde guardar CSVs
    
    # ValidaciÃ³n
    MIN_DATA_POINTS = 100  # MÃ­nimo de dÃ­as para considerar dataset vÃ¡lido
    MAX_NAN_PERCENT = 5.0  # % mÃ¡ximo de NaN aceptable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNCIONES PRINCIPALES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def download_data(ticker: str, start_date: str, end_date: str, 
                  interval: str = '1d') -> pd.DataFrame:
    """
    Descarga datos histÃ³ricos de un ticker usando yfinance.
    
    Args:
        ticker: SÃ­mbolo del activo (ej: 'SPY', 'AAPL', 'BTC-USD')
        start_date: Fecha inicio en formato 'YYYY-MM-DD'
        end_date: Fecha fin en formato 'YYYY-MM-DD'
        interval: Timeframe ('1d', '1h', etc.)
    
    Returns:
        DataFrame con columnas OHLCV
    
    Raises:
        ValueError: Si no se pueden descargar datos
    """
    print(f"ğŸ“¥ Descargando {ticker} desde {start_date} hasta {end_date}...")
    
    try:
        # Descargar usando yfinance
        df = yf.download(
            ticker,
            start=start_date,
            end=end_date,
            interval=interval,
            progress=False,  # Silenciar barra de progreso
            auto_adjust=True  # Ajustar por splits y dividendos
        )
        
        # Validar que se descargaron datos
        if df.empty:
            raise ValueError(f"No se encontraron datos para {ticker}")
        
        # Normalizar nombres de columnas (yfinance puede retornar con mayÃºsculas)
        df.columns = [col.lower() for col in df.columns]
        
        print(f"âœ… Descargados {len(df)} registros de {ticker}")
        return df
        
    except Exception as e:
        print(f"âŒ Error descargando {ticker}: {str(e)}")
        raise ValueError(f"No se pudo descargar {ticker}")


def clean_data(df: pd.DataFrame, ticker: str) -> pd.DataFrame:
    """
    Limpia datos eliminando NaN, duplicados, y validando estructura.
    
    Args:
        df: DataFrame con datos descargados
        ticker: Nombre del ticker (para logging)
    
    Returns:
        DataFrame limpio
    """
    print(f"ğŸ§¹ Limpiando datos de {ticker}...")
    
    # Guardar tamaÃ±o original para reporte
    original_len = len(df)
    
    # 1. Eliminar duplicados en el Ã­ndice (fechas duplicadas)
    df = df[~df.index.duplicated(keep='first')]
    duplicates_removed = original_len - len(df)
    if duplicates_removed > 0:
        print(f"   âš ï¸  Eliminados {duplicates_removed} registros duplicados")
    
    # 2. Verificar columnas requeridas
    required_cols = ['open', 'high', 'low', 'close', 'volume']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Faltan columnas requeridas: {missing_cols}")
    
    # 3. Convertir columnas a tipo numÃ©rico (por si acaso)
    for col in required_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # 4. Eliminar filas donde TODAS las columnas son NaN
    df = df.dropna(how='all')
    
    # 5. Para datos faltantes parciales, hacer forward fill (mÃ¡x 2 dÃ­as)
    # Rationale: Gaps de 1-2 dÃ­as son normales (fines de semana, feriados)
    nan_before = df.isnull().sum().sum()
    df = df.fillna(method='ffill', limit=2)
    nan_after = df.isnull().sum().sum()
    
    if nan_before > 0:
        print(f"   âš ï¸  Rellenados {nan_before - nan_after} valores NaN con forward fill")
    
    # 6. Si aÃºn quedan NaN despuÃ©s de ffill, eliminar esas filas
    rows_before = len(df)
    df = df.dropna()
    rows_dropped = rows_before - len(df)
    
    if rows_dropped > 0:
        print(f"   âš ï¸  Eliminadas {rows_dropped} filas con NaN persistentes")
    
    # 7. Validar lÃ³gica OHLC (High >= Low, etc.)
    invalid_ohlc = df[
        (df['high'] < df['low']) | 
        (df['close'] > df['high']) | 
        (df['close'] < df['low'])
    ]
    
    if len(invalid_ohlc) > 0:
        print(f"   âš ï¸  Encontradas {len(invalid_ohlc)} filas con OHLC invÃ¡lido (eliminadas)")
        df = df.drop(invalid_ohlc.index)
    
    # 8. Eliminar filas con precio = 0 (datos corruptos)
    zero_prices = df[(df['close'] == 0) | (df['high'] == 0)]
    if len(zero_prices) > 0:
        print(f"   âš ï¸  Eliminadas {len(zero_prices)} filas con precio = 0")
        df = df.drop(zero_prices.index)
    
    print(f"âœ… Limpieza completada: {len(df)} registros vÃ¡lidos ({original_len - len(df)} eliminados)")
    return df


def validate_data(df: pd.DataFrame, ticker: str, 
                  min_points: int = 100, 
                  max_nan_percent: float = 5.0) -> dict:
    """
    Valida calidad de datos y genera reporte.
    
    Args:
        df: DataFrame limpio
        ticker: Nombre del ticker
        min_points: MÃ­nimo de registros requeridos
        max_nan_percent: % mÃ¡ximo de NaN aceptable
    
    Returns:
        dict con estadÃ­sticas de validaciÃ³n
    """
    print(f"âœ… Validando calidad de datos de {ticker}...")
    
    # Inicializar reporte
    report = {
        'ticker': ticker,
        'total_records': len(df),
        'date_range': f"{df.index.min()} to {df.index.max()}",
        'is_valid': True,
        'errors': [],
        'warnings': []
    }
    
    # ValidaciÃ³n 1: Cantidad mÃ­nima de datos
    if len(df) < min_points:
        report['is_valid'] = False
        report['errors'].append(
            f"Muy pocos datos: {len(df)} < {min_points} requeridos"
        )
    
    # ValidaciÃ³n 2: Porcentaje de NaN
    nan_percent = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
    if nan_percent > max_nan_percent:
        report['is_valid'] = False
        report['errors'].append(
            f"Demasiados NaN: {nan_percent:.2f}% > {max_nan_percent}% lÃ­mite"
        )
    
    # ValidaciÃ³n 3: Gaps grandes en fechas (solo para datos diarios)
    if '1d' in str(df.index.freq) or df.index.freq is None:
        date_diff = df.index.to_series().diff()
        # Gaps >10 dÃ­as son sospechosos (excluyendo inicio)
        large_gaps = date_diff[date_diff > timedelta(days=10)].iloc[1:]
        
        if len(large_gaps) > 0:
            report['warnings'].append(
                f"Detectados {len(large_gaps)} gaps >10 dÃ­as en fechas"
            )
    
    # ValidaciÃ³n 4: Volatilidad extrema (posible dato corrupto)
    returns = df['close'].pct_change()
    extreme_returns = returns[abs(returns) > 0.5]  # >50% cambio en 1 dÃ­a
    
    if len(extreme_returns) > 0:
        report['warnings'].append(
            f"Detectados {len(extreme_returns)} cambios de precio >50% (revisar)"
        )
    
    # EstadÃ­sticas adicionales
    report['stats'] = {
        'avg_volume': f"{df['volume'].mean():,.0f}",
        'avg_price': f"${df['close'].mean():.2f}",
        'price_range': f"${df['close'].min():.2f} - ${df['close'].max():.2f}",
        'nan_count': int(df.isnull().sum().sum())
    }
    
    # Imprimir resultado
    if report['is_valid']:
        print(f"   âœ… ValidaciÃ³n EXITOSA")
    else:
        print(f"   âŒ ValidaciÃ³n FALLIDA:")
        for error in report['errors']:
            print(f"      - {error}")
    
    if report['warnings']:
        print(f"   âš ï¸  Advertencias:")
        for warning in report['warnings']:
            print(f"      - {warning}")
    
    return report


def save_to_csv(df: pd.DataFrame, ticker: str, output_dir: str = './data/'):
    """
    Guarda DataFrame a CSV para reutilizaciÃ³n.
    
    Args:
        df: DataFrame a guardar
        ticker: Nombre del ticker
        output_dir: Directorio de salida
    """
    import os
    
    # Crear directorio si no existe
    os.makedirs(output_dir, exist_ok=True)
    
    # Generar nombre de archivo
    filename = f"{ticker}_{datetime.now().strftime('%Y%m%d')}.csv"
    filepath = os.path.join(output_dir, filename)
    
    # Guardar
    df.to_csv(filepath)
    print(f"ğŸ’¾ Guardado en: {filepath}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE COMPLETO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_pipeline(tickers: list, start_date: str, end_date: str, 
                 interval: str = '1d', output_dir: str = './data/') -> dict:
    """
    Ejecuta pipeline completo para mÃºltiples tickers.
    
    Args:
        tickers: Lista de tickers a procesar
        start_date: Fecha inicio
        end_date: Fecha fin
        interval: Timeframe
        output_dir: Directorio de salida
    
    Returns:
        dict con resultados por ticker
    """
    print("\n" + "="*80)
    print("ğŸš€ INICIANDO DATA PIPELINE")
    print("="*80 + "\n")
    
    results = {}
    
    for ticker in tickers:
        print(f"\n{'â”€'*80}")
        print(f"Procesando: {ticker}")
        print(f"{'â”€'*80}")
        
        try:
            # 1. Descargar
            df = download_data(ticker, start_date, end_date, interval)
            
            # 2. Limpiar
            df = clean_data(df, ticker)
            
            # 3. Validar
            validation = validate_data(
                df, ticker, 
                min_points=DataConfig.MIN_DATA_POINTS,
                max_nan_percent=DataConfig.MAX_NAN_PERCENT
            )
            
            # 4. Guardar si es vÃ¡lido
            if validation['is_valid']:
                save_to_csv(df, ticker, output_dir)
                results[ticker] = {
                    'status': 'success',
                    'data': df,
                    'validation': validation
                }
            else:
                results[ticker] = {
                    'status': 'failed_validation',
                    'validation': validation
                }
            
        except Exception as e:
            print(f"âŒ Error procesando {ticker}: {str(e)}")
            results[ticker] = {
                'status': 'error',
                'error': str(e)
            }
    
    # Reporte final
    print("\n" + "="*80)
    print("ğŸ“Š RESUMEN FINAL")
    print("="*80)
    
    success = sum(1 for r in results.values() if r['status'] == 'success')
    failed = len(results) - success
    
    print(f"âœ… Exitosos: {success}/{len(results)}")
    print(f"âŒ Fallidos: {failed}/{len(results)}")
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EJEMPLO DE USO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    """
    Ejemplo de cÃ³mo usar este script.
    """
    
    # OpciÃ³n 1: Usar configuraciÃ³n por defecto
    results = run_pipeline(
        tickers=DataConfig.TICKERS,
        start_date=DataConfig.START_DATE,
        end_date=DataConfig.END_DATE,
        interval=DataConfig.INTERVAL,
        output_dir=DataConfig.OUTPUT_DIR
    )
    
    # OpciÃ³n 2: Personalizar parÃ¡metros
    # results = run_pipeline(
    #     tickers=['AAPL', 'MSFT', 'GOOGL'],
    #     start_date='2023-01-01',
    #     end_date='2024-01-01',
    #     interval='1h',  # Datos horarios
    #     output_dir='./mi_carpeta_datos/'
    # )
    
    # Acceder a datos de un ticker especÃ­fico
    if 'SPY' in results and results['SPY']['status'] == 'success':
        spy_data = results['SPY']['data']
        print(f"\nğŸ“ˆ Primeras 5 filas de SPY:")
        print(spy_data.head())
        
        print(f"\nğŸ“Š EstadÃ­sticas de SPY:")
        print(spy_data.describe())


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTAS PARA EL USUARIO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
ğŸ“š GUÃA DE USO RÃPIDO:

1. INSTALACIÃ“N DE DEPENDENCIAS:
   pip install yfinance pandas numpy

2. EJECUCIÃ“N BÃSICA:
   python data_pipeline_simple.py

3. PERSONALIZACIÃ“N:
   - Edita la clase DataConfig para cambiar tickers/fechas
   - O llama a run_pipeline() con tus propios parÃ¡metros

4. OUTPUT:
   - CSVs guardados en ./data/ por defecto
   - Formato: TICKER_YYYYMMDD.csv

5. TIMEFRAMES DISPONIBLES:
   - Intraday: '1m', '5m', '15m', '30m', '1h'
   - Daily y mÃ¡s: '1d', '1wk', '1mo'

âš ï¸ LIMITACIONES DE YFINANCE:
   - Datos intraday: MÃ¡ximo 60 dÃ­as de historial
   - Datos diarios: Sin lÃ­mite (aÃ±os disponibles)
   - Criptos: Usar formato 'BTC-USD', 'ETH-USD'
   - Forex: Usar formato 'EURUSD=X'

ğŸ“ SIGUIENTE PASO:
   Usa estos datos limpios en tu backtest (SesiÃ³n 5 del workshop)

ğŸ”— EN EL WORKSHOP PREMIUM:
   - data_pipeline_advanced.py (con caching, multi-threading)
   - IntegraciÃ³n con Alpaca, Interactive Brokers, Polygon
   - Auto-detecciÃ³n de splits/dividendos
   - NormalizaciÃ³n multi-fuente (combinar yfinance + Alpaca)
"""
